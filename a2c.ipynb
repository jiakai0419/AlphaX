{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.math.exp(tf.math.log(2.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3.0\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.17.3\n",
    "gym.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_ID = 'CartPole-v0'\n",
    "ENV_UNWRAPPED = False\n",
    "ENV_REWARD_THRESHOLD = 195.0\n",
    "\n",
    "GAMMA = 0.99\n",
    "\n",
    "ENTROPY_LOSS_COEF = 0.01\n",
    "\n",
    "CONSECUTIVE_TRIALS = 100\n",
    "\n",
    "TEST_TIMES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(ENV_ID)\n",
    "\n",
    "if ENV_UNWRAPPED:\n",
    "    env = env.unwrapped\n",
    "\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "action_num = env.action_space.n\n",
    "\n",
    "print('obs_dim', obs_dim)\n",
    "print('action_num', action_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = tf.keras.Sequential([\n",
    "    layers.Dense(16, activation='tanh'),\n",
    "    layers.Dense(16, activation='tanh'),\n",
    "    layers.Dense(action_num)\n",
    "])\n",
    "\n",
    "policy_optimizer=tf.optimizers.Adam(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_network = tf.keras.Sequential([\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "value_network_optimizer=tf.optimizers.Adam(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_to_go(rewards):\n",
    "    n = len(rewards)\n",
    "    returns = np.zeros_like(rewards, dtype=np.float32)\n",
    "    for i in reversed(range(n)):\n",
    "        returns[i] = rewards[i] + GAMMA * (0.0 if i == n-1 else returns[i+1])\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_advantage(observations, rewards):\n",
    "    assert(len(observations) == len(rewards))\n",
    "    \n",
    "    n = len(observations)\n",
    "    \n",
    "    values = []\n",
    "    for i in range(n):\n",
    "        values.append(\n",
    "            value_network(tf.expand_dims(observations[i], 0))\n",
    "        )\n",
    "    \n",
    "    advantages = np.zeros_like(rewards, dtype=np.float32)\n",
    "    td_targets = np.zeros_like(rewards, dtype=np.float32)\n",
    "    for i in range(n):\n",
    "        bootstrap = 0.0 if i == n-1 else tf.stop_gradient(values[i+1])\n",
    "        td_targets[i] = rewards[i] + GAMMA * bootstrap\n",
    "        advantages[i] = td_targets[i] - tf.stop_gradient(values[i])\n",
    "    \n",
    "    return advantages, values, td_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_episode():\n",
    "    ep_obs = []\n",
    "    ep_action = []\n",
    "    ep_reward = []\n",
    "    ep_log_prob = []\n",
    "    ep_entropy = []\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        obs = env.reset()\n",
    "        \n",
    "        while True:\n",
    "            ep_obs.append(obs.copy())\n",
    "\n",
    "            policy_logits = policy(tf.expand_dims(obs, 0))\n",
    "            log_prob = tf.nn.log_softmax(policy_logits)\n",
    "            entropy = -tf.reduce_sum(tf.math.exp(log_prob) * log_prob)\n",
    "            action = tf.squeeze(tf.random.categorical(policy_logits, 1, seed=None), axis=1)[0]\n",
    "\n",
    "            obs, reward, done, _ = env.step(action.numpy())\n",
    "\n",
    "            ep_action.append(action)\n",
    "            ep_reward.append(reward)\n",
    "            ep_log_prob.append(log_prob)\n",
    "            ep_entropy.append(entropy)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        ep_advantage, ep_estimated_value, ep_td_target = estimate_advantage(ep_obs, ep_reward)  # [batch_size]\n",
    "        action_mask = tf.one_hot(ep_action, action_num)  # [batch_size, action_num]\n",
    "        log_probs = tf.reduce_sum(action_mask * tf.concat(ep_log_prob, axis=0), axis=1)  # [batch_size]\n",
    "        policy_loss = -(tf.reduce_mean(ep_advantage * log_probs) + ENTROPY_LOSS_COEF * tf.reduce_mean(ep_entropy))\n",
    "        policy_loss_0 = -tf.reduce_mean(ep_advantage * log_probs)\n",
    "        policy_loss_1 = -ENTROPY_LOSS_COEF * tf.reduce_mean(ep_entropy)\n",
    "        value_network_loss = tf.reduce_mean(\n",
    "            tf.math.square(\n",
    "                # MC\n",
    "                #tf.expand_dims(reward_to_go(ep_reward), 1) - tf.concat(ep_estimated_value, axis=0)\n",
    "                \n",
    "                # TD\n",
    "                tf.expand_dims(ep_td_target, 1) - tf.concat(ep_estimated_value, axis=0)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    policy_grads = tape.gradient(policy_loss, policy.trainable_variables)\n",
    "    policy_optimizer.apply_gradients(zip(policy_grads, policy.trainable_variables))\n",
    "    \n",
    "    value_network_grads = tape.gradient(value_network_loss, value_network.trainable_variables)\n",
    "    value_network_optimizer.apply_gradients(zip(value_network_grads, value_network.trainable_variables))\n",
    "\n",
    "    del tape\n",
    "    \n",
    "    return sum(ep_reward), len(ep_reward), policy_loss, policy_loss_0, policy_loss_1, value_network_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [0.0] * CONSECUTIVE_TRIALS\n",
    "i = 0\n",
    "while True:\n",
    "    total_rewards, episode_len, policy_loss, policy_loss_0, policy_loss_1, value_network_loss = train_one_episode()\n",
    "    results[i % CONSECUTIVE_TRIALS] = total_rewards\n",
    "    avg_results = np.average(results) if CONSECUTIVE_TRIALS <= i else np.average(results[0:i+1])\n",
    "    print('i={}, total_rewards={}, episode_len={}, p_loss={:.4f}, p_loss[0]={:.4f}, p_loss[1]={:.4f}, v_loss={:.4f}, avg_results={}'.format(\n",
    "        i, total_rewards, episode_len, policy_loss, policy_loss_0, policy_loss_1, value_network_loss, avg_results))\n",
    "    if ENV_REWARD_THRESHOLD <= avg_results:\n",
    "        print('congratulations!')\n",
    "        break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(TEST_TIMES):\n",
    "    total_rewards = 0\n",
    "    obs = env.reset()\n",
    "    while True:\n",
    "        env.render()\n",
    "        policy_logits = policy(tf.expand_dims(obs, 0))\n",
    "        action = tf.squeeze(tf.random.categorical(policy_logits, 1, seed=None), axis=1)[0]\n",
    "        obs, reward, done, _ = env.step(action.numpy())\n",
    "        total_rewards += reward\n",
    "        if done:\n",
    "            break\n",
    "    print('i:{}, total_rewards:{}'.format(i, total_rewards))\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF 2.3.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
